# ğŸ¤– KI Agent E2E Testing - COMPLETE GUIDE

**How to Test MCP Multi KI Agents that Generate Software**

---

## ğŸ“Š OVERVIEW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  AGENT E2E TESTING = WebSocket + Workflow Validation      â”‚
â”‚                                                            â”‚
â”‚  Different from App E2E Testing!                          â”‚
â”‚                                                            â”‚
â”‚  App E2E:    Test the app that users interact with        â”‚
â”‚  Agent E2E:  Test the agent that creates apps             â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ WHAT IS AGENT E2E TESTING?

### The Agent Workflow

```
User Request
    â†“
WebSocket Message
    â†“
Backend Server (port 8002)
    â†“
Supervisor Agent (parses & routes)
    â†“
Codesmith Agent (creates structure)
    â†“
ComponentWriter Agent (generates code)
    â†“
E2E Generator Agent (creates tests)
    â†“
ReviewFix Agent (validates & fixes)
    â†“
Generated App Files in Workspace
    â†“
âœ… Success / âŒ Failure
```

### What We Test

| Component | Test | Validates |
|-----------|------|-----------|
| **Connection** | WebSocket establishes | Agent reachable |
| **Request** | Send message to agent | Agent receives it |
| **Processing** | Agents execute workflow | Correct routing |
| **Output** | Receive response messages | Agent responds |
| **Artifacts** | Files generated | App created |
| **Location** | Files in correct workspace | Isolation verified |
| **Content** | Generated code quality | Syntax & structure |
| **Tests** | E2E tests generated | Test coverage |

---

## ğŸš€ THREE WAYS TO TEST

### 1. Manual Interactive Test (Development & Debugging)

```bash
python test_agent_manual_interactive.py
```

**Use When:**
- Developing new agent features
- Debugging issues
- Understanding workflow
- Manual validation

**Features:**
- âœ… Interactive menu with scenarios
- âœ… Real-time agent response display
- âœ… File generation monitoring
- âœ… Colored, readable output
- âœ… Choose custom requests

**Workflow:**
```
1. Script starts
2. Workspace created (~TestApps/...)
3. Connect to backend
4. Show test scenarios (1, 2, 3, custom)
5. Execute selected scenario
6. Monitor responses in real-time
7. Show generated files
8. Ask for another test
```

---

### 2. Automated E2E Test (CI/CD & Validation)

```bash
python test_agent_websocket_real_e2e.py
```

**Use When:**
- Continuous Integration
- Automated validation
- Performance testing
- Regression detection

**Features:**
- âœ… Fully automated
- âœ… 7-phase validation
- âœ… Comprehensive checks
- âœ… Detailed reporting
- âœ… Exit codes (0=pass, 1=fail)

**Phases:**
```
PHASE 1: Setup
  âœ“ Workspace clean
  âœ“ Backend ready

PHASE 2: Connect
  âœ“ WebSocket established
  âœ“ Init acknowledged

PHASE 3: Request
  âœ“ App request sent
  âœ“ Message ID tracked

PHASE 4: Monitor
  âœ“ Responses received
  âœ“ Timeout handled

PHASE 5: Validate
  âœ“ No errors
  âœ“ Workflow complete

PHASE 6: Verify
  âœ“ Files generated
  âœ“ Structure valid

PHASE 7: Summary
  âœ“ Metrics reported
```

---

### 3. Manual WebSocket Test (Direct Testing)

```bash
# Connect directly
wscat -c ws://localhost:8002/ws/chat

# Send init
{"type":"init","workspace_path":"/tmp/test"}

# Send request
{"type":"message","content":"Create React app"}

# Monitor responses
[receive messages...]
```

**Use When:**
- Direct protocol testing
- Low-level debugging
- Protocol validation

---

## ğŸ“‹ QUICK START (5 MINUTES)

### Step 1: Backend Server

```bash
# Terminal 1
cd /Users/dominikfoert/git/KI_AutoAgent
python start_server.py --port=8002

# Wait for: âœ“ Server started on port 8002
```

### Step 2: Run Test

```bash
# Terminal 2
cd /Users/dominikfoert/git/KI_AutoAgent
python test_agent_manual_interactive.py
```

### Step 3: Select & Watch

```
1. Simple React Todo App
2. React Dashboard
3. Contact Form
4. Custom Request

Select: 1

[Watch agent generate app...]

âœ… Complete!
```

---

## ğŸ” WHAT GETS VALIDATED

### Phase 1: Environment
```
âœ“ Workspace outside dev repo
âœ“ Workspace is clean (no old files)
âœ“ Backend server running
âœ“ Port 8002 available
âœ“ No stale processes
```

### Phase 2: Connection
```
âœ“ WebSocket connects
âœ“ Connection timeout < 5s
âœ“ Init message sent
âœ“ Init acknowledged with success=true
```

### Phase 3: Request
```
âœ“ Request formatted correctly
âœ“ Message ID assigned
âœ“ Request sent to agent
âœ“ Agent receives it
```

### Phase 4: Execution
```
âœ“ Agent responds (first msg < 5s)
âœ“ Multiple messages received (> 10)
âœ“ Message types vary: status, progress, output
âœ“ No critical errors
âœ“ Workflow progresses logically
```

### Phase 5: Results
```
âœ“ Workflow completes (COMPLETE message)
âœ“ No unhandled exceptions
âœ“ Response consistent
âœ“ Timing reasonable (< 120s)
```

### Phase 6: Artifacts
```
âœ“ Files generated (> 20 files)
âœ“ Files in correct workspace
âœ“ Files NOT in dev repo
âœ“ App structure correct:
  âœ“ package.json exists
  âœ“ src/ directory exists
  âœ“ README.md exists
```

### Phase 7: Quality
```
âœ“ Generated code has valid syntax
âœ“ JSON files are valid
âœ“ No suspicious content (undefined, [object Object])
âœ“ No executable vulnerabilities
âœ“ File permissions correct
```

---

## ğŸ“ AGENT TESTING PATTERNS

### Pattern 1: Request â†’ Response

```python
# What we do:
async def test():
    client = E2EWebSocketClient(ws_url, workspace)
    await client.connect()
    await client.send_request("Create React app")
    messages = await client.receive_all_messages()
    assert len(messages) > 0
    assert messages[-1]["type"] == "complete"
```

### Pattern 2: State Tracking

```python
# What we do:
state_trace = []
for msg in messages:
    state_trace.append({
        "timestamp": msg["timestamp"],
        "agent": msg.get("agent"),
        "status": msg.get("status"),
        "artifact": msg.get("artifact")
    })

# Verify state transitions are logical
```

### Pattern 3: Artifact Validation

```python
# What we do:
generated_files = list(workspace.rglob("*"))

# Validate structure
assert (workspace / "package.json").exists()
assert (workspace / "src").is_dir()
assert len(generated_files) > 20

# Validate content
for file in generated_files:
    if file.suffix in [".js", ".jsx", ".json"]:
        validate_syntax(file)
```

### Pattern 4: Error Handling

```python
# What we do:
for msg in messages:
    if msg["type"] == "error":
        logger.error(f"Agent error: {msg['content']}")
        test_failed = True

assert not test_failed, "Agent encountered errors"
```

---

## âœ… SUCCESS CRITERIA

### Test Passes If:

```
âœ… Connection established
âœ… At least 10 messages received
âœ… No ERROR type messages
âœ… Workflow completed (type="complete")
âœ… 30+ files generated
âœ… Files in ~/TestApps/... (not dev repo)
âœ… package.json valid JSON
âœ… src/ directory exists
âœ… No suspicious content in generated code
âœ… Total execution time < 120 seconds
```

### Test Fails If:

```
âŒ Connection timeout
âŒ 0 messages received
âŒ ERROR message found
âŒ No complete message
âŒ No files generated
âŒ Files in development repo
âŒ Invalid JSON
âŒ Missing directories
âŒ Garbage in generated code
âŒ Execution > 300 seconds
```

**Rule:** ANY failure = Test FAILED. No partial credit!

---

## ğŸ› DEBUGGING WORKFLOW

### Issue: Connection Refused

**Root Cause:** Backend not running on port 8002

**Debug:**
```bash
# Check if running
ps aux | grep start_server

# Check port
lsof -i :8002

# Check if process crashed
tail -20 /tmp/v7_server.log
```

**Fix:**
```bash
# Kill any zombie processes
pkill -f "start_server"

# Restart
python start_server.py --port=8002
```

---

### Issue: No Messages Received

**Root Cause:** Agent not processing request

**Debug:**
```bash
# Check backend logs
tail -50 /tmp/v7_server.log | grep -i "error\|exception"

# Check WebSocket connection
python -c "
import asyncio, websockets
async def test():
    try:
        async with websockets.connect('ws://localhost:8002/ws/chat') as ws:
            await ws.send('{\"type\":\"init\",\"workspace_path\":\"/tmp/t\"}')
            print(await ws.recv())
    except Exception as e:
        print(f'Error: {e}')
asyncio.run(test())
"
```

**Fix:**
- Check API key in `.env`
- Check rate limits
- Check request format
- Restart backend

---

### Issue: Files in Dev Repo

**Root Cause:** Wrong workspace_path or subprocess cwd

**Debug:**
```bash
# Check backend code
grep -r "workspace_path" backend/agents/

# Check if cwd is set in subprocess
grep -r "cwd=" backend/adapters/

# Verify sent workspace
grep "workspace_path" /tmp/v7_server.log
```

**Fix:**
- Ensure `workspace_path` sent in init
- Ensure backend receives it
- Ensure subprocess uses `cwd=workspace_path`
- Clean both ~/TestApps/ and dev repo

---

### Issue: Agent Timeout

**Root Cause:** Agent hung or very slow

**Debug:**
```bash
# Check if process is stuck
ps aux | grep -i "python\|claude"

# Check for Claude API rate limit
grep -i "rate\|limit\|429" /tmp/v7_server.log

# Monitor system resources
top -n5 -b | head -20
```

**Fix:**
- Wait longer (agent may be slow)
- Check Claude API dashboard
- Increase timeout value
- Restart backend

---

## ğŸ“Š MONITORING DURING TEST

### Live Backend Monitoring

```bash
# Terminal 3: Watch logs
tail -f /tmp/v7_server.log
```

**Look for:**
```
âœ“ Connection received from client
âœ“ Init message received
âœ“ workspace_path: /Users/.../TestApps/...
âœ“ Supervisor processing request
âœ“ Codesmith creating structure
âœ“ ComponentWriter generating code
âœ“ E2E generator creating tests
âœ“ ReviewFix validating
âœ“ Workflow completed
âœ“ Files written to workspace
```

### Live File Monitoring

```bash
# Terminal 4: Watch file generation
watch -n 1 'find ~/TestApps -type f | wc -l'

# Or with tree
tree -L 3 ~/TestApps
```

---

## ğŸ”„ TESTING CYCLE

### Development Loop

```
1. Make change to agent code
   â””â”€ backend/agents/supervisor_agent.py

2. Restart backend
   â””â”€ python start_server.py --port=8002

3. Run test
   â””â”€ python test_agent_manual_interactive.py

4. Observe output
   â””â”€ Check terminal output & backend logs

5. Verify results
   â””â”€ Check generated files

6. If passed: âœ… Deploy
   If failed: âŒ Fix code, goto 1
```

---

## ğŸ“ˆ PERFORMANCE EXPECTATIONS

| Metric | Expected | Max |
|--------|----------|-----|
| Connection | < 1s | 5s |
| First response | < 5s | 10s |
| Full execution | 60-120s | 300s |
| Messages | 15-50 | unlimited |
| Files | 30-100 | unlimited |
| File size | 1-100 KB | unlimited |

---

## ğŸ¯ TEST SCENARIOS

### Scenario 1: React Todo App

```
Request:
Create a React Todo Application with:
- Input to add todos
- Display list
- Mark complete
- Delete functionality
- Local storage

Expected:
- React components generated
- Todo logic implemented
- Tests created
- ~40-50 files
- ~5 minutes
```

### Scenario 2: React Dashboard

```
Request:
Create React Dashboard with:
- Grid layout
- Data cards
- Chart
- Dark mode
- Statistics

Expected:
- Dashboard components
- Chart library integration
- Theme system
- ~50-60 files
- ~6 minutes
```

### Scenario 3: FastAPI Backend

```
Request:
Create FastAPI with:
- User CRUD
- Authentication
- Rate limiting
- API docs

Expected:
- FastAPI routes
- Database models
- Auth middleware
- Tests
- ~30-40 files
- ~5 minutes
```

---

## ğŸš¨ CRITICAL RULES

### Rule 1: Workspace Isolation
```
âŒ NEVER: ~/git/KI_AutoAgent/test_output/
âœ… ALWAYS: ~/TestApps/...
```

### Rule 2: Clean Before Test
```bash
# Always clean first!
rm -rf ~/TestApps/*
```

### Rule 3: No Exceptions
```
âœ… Test passes: All criteria met
âŒ Test fails: ANY error found
(No partial success)
```

### Rule 4: Verify Location
```bash
# After test, check:
ls -la ~/TestApps/
# Should have generated app

# Check dev repo:
find /Users/dominikfoert/git/KI_AutoAgent -name "*app" -maxdepth 1
# Should be EMPTY
```

---

## ğŸ“š DOCUMENTATION FILES

| File | Purpose |
|------|---------|
| **test_agent_websocket_real_e2e.py** | Automated E2E test |
| **test_agent_manual_interactive.py** | Interactive test |
| **AGENT_E2E_TEST_QUICK_START.md** | Quick start guide |
| **AGENT_TESTING_CHECKLIST.md** | Complete checklist |
| **E2E_TESTING_GUIDE.md** | Best practices |
| **CRITICAL_FAILURE_INSTRUCTIONS.md** | Error handling |

---

## ğŸ“ EXAMPLES

### Example 1: Successful Test

```
$ python test_agent_manual_interactive.py

ğŸ”— Connecting to agent...
âœ… Connected!

â•â•â• Test Scenarios â•â•â•
1. Simple React Todo App
2. React Dashboard
3. Contact Form

Select: 1
ğŸ“¤ Sending request...

ğŸ“¨ Monitoring...
â„¹ï¸  Status: Processing...
â³ Supervisor analyzing...
âœ“ ComponentWriter: Generated App.jsx
âœ“ ComponentWriter: Generated TodoList.jsx
âœ“ E2E Generator: Generated App.test.js
âœ… COMPLETE!

ğŸ“ Generated Files:
   Total: 45 files
   .jsx: 8
   .json: 3
   .js: 12

Test another? (y/n): n

âœ… Test passed!
```

### Example 2: Failed Test

```
$ python test_agent_websocket_real_e2e.py

ğŸ“‹ PHASE 1: SETUP
âŒ Workspace not clean! Found: [old_app/]

Fix: rm -rf ~/TestApps/*

âŒ E2E TEST FAILED!
Exit code: 1
```

---

## â“ FAQ

**Q: Can I run multiple tests simultaneously?**  
A: Yes, on different ports (8002, 8003, 8004) and workspaces

**Q: How long does a test take?**  
A: 60-120 seconds normally

**Q: What if backend crashes?**  
A: Restart with `python start_server.py --port=8002`

**Q: Where are generated files?**  
A: `~/TestApps/manual_interactive_test/TIMESTAMP/APPNAME/`

**Q: Can I keep generated files after test?**  
A: Yes, they're in ~/TestApps/ automatically

**Q: How do I integrate into CI/CD?**  
A: Use `test_agent_websocket_real_e2e.py`, check exit code

**Q: What's the difference from app E2E testing?**  
A: Agent E2E tests the agent itself, not the generated app

---

## ğŸ¯ NEXT STEPS

### If Test Passed âœ…
1. Great! Agent is working
2. Proceed with production deployment
3. Run regularly for regression detection

### If Test Failed âŒ
1. Identify failure phase (see checklist)
2. Follow debugging steps
3. Fix identified issue
4. Run test again

### For Continuous Testing
1. Add to CI/CD pipeline
2. Run on every commit
3. Alert on failures
4. Track metrics over time

---

**Version**: 1.0  
**Status**: READY FOR PRODUCTION âœ…  
**Last Updated**: 2025-02-15

---

## ğŸ“ Support

For issues:
1. Check AGENT_TESTING_CHECKLIST.md
2. Review CRITICAL_FAILURE_INSTRUCTIONS.md
3. Check backend logs: `tail -f /tmp/v7_server.log`
4. Run with `--debug` flag for verbose output