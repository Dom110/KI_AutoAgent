"""
Research Subgraph v6.1 - Custom Node Implementation

This is a refactored version that doesn't use create_react_agent,
allowing it to work with Claude CLI adapter.

Changes from v6.0:
- Removed create_react_agent (incompatible with async-only LLMs)
- Direct LLM.ainvoke() calls (like Architect pattern)
- Manual tool calling for Perplexity (simplified)
- Works with ClaudeCLISimple adapter

Author: KI AutoAgent Team
Python: 3.13+
"""

from __future__ import annotations

import logging
from datetime import datetime
from typing import Any

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import END, StateGraph

from state_v6 import ResearchState
from tools.perplexity_tool import perplexity_search

logger = logging.getLogger(__name__)


def create_research_subgraph(
    workspace_path: str,
    memory: Any | None = None
) -> Any:
    """
    Create Research subgraph with custom node implementation.

    This version uses direct LLM calls instead of create_react_agent,
    making it compatible with async-only LLMs like ClaudeCLISimple.

    Args:
        workspace_path: Path to workspace
        memory: Memory system instance (optional)

    Returns:
        Compiled research subgraph
    """
    logger.debug("Creating Research subgraph v6.1 (custom node)...")

    # Research node function
    async def research_node(state: ResearchState) -> ResearchState:
        """
        Execute research with custom implementation.

        Flow:
        1. Use Perplexity to search for information
        2. Use Claude to analyze and summarize findings
        3. Store in Memory
        4. Return results
        """
        logger.info(f"üîç Research node v6.1 executing: {state['query']}")

        try:
            # Step 1: Search with Perplexity
            logger.info("üåê Searching with Perplexity...")
            search_result = await perplexity_search.ainvoke({"query": state['query']})

            search_findings = search_result.get("content", "No results found")
            logger.info(f"‚úÖ Perplexity results: {len(search_findings)} chars")

            # Step 2: Analyze with Claude
            logger.info("ü§ñ Analyzing findings with Claude...")

            llm = ChatAnthropic(
                model="claude-sonnet-4-20250514",
                temperature=0.3,
                max_tokens=4096
            )

            system_prompt = """You are a research analyst specializing in software development.

Your responsibilities:
1. Analyze search results and extract key insights
2. Identify relevant technologies, patterns, and best practices
3. Summarize findings concisely
4. Provide actionable recommendations

Output format:
- Key Findings: Main insights from the research
- Technologies: Relevant tools/frameworks mentioned
- Best Practices: Recommended approaches
- Sources: Where the information came from"""

            user_prompt = f"""Analyze the following research results:

**Query:** {state['query']}

**Search Results:**
{search_findings}

Provide a structured summary of the key findings."""

            response = await llm.ainvoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ])

            analysis = response.content if hasattr(response, 'content') else str(response)
            logger.info(f"‚úÖ Analysis complete: {len(analysis)} chars")

            # Step 3: Create research report
            report = f"""# Research Report

**Query:** {state['query']}
**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Analysis

{analysis}

## Raw Search Results

{search_findings[:500]}...

---
*Generated by Research Agent v6.1*
"""

            findings = {
                "analysis": analysis,
                "raw_results": search_findings,
                "timestamp": datetime.now().isoformat()
            }

            # Step 4: Store in Memory (if available)
            if memory:
                logger.info("üíæ Storing findings in Memory...")
                await memory.store(
                    content=analysis,
                    metadata={
                        "agent": "research",
                        "type": "findings",
                        "query": state['query'],
                        "timestamp": findings["timestamp"]
                    }
                )
                logger.debug("‚úÖ Findings stored in Memory")

            # Return updated state
            return {
                **state,
                "findings": findings,
                "report": report,
                "completed": True,
                "errors": []
            }

        except Exception as e:
            logger.error(f"‚ùå Research node failed: {e}", exc_info=True)

            return {
                **state,
                "findings": None,
                "report": f"Research failed: {str(e)}",
                "completed": False,
                "errors": [{"error": str(e), "node": "research"}]
            }

    # Build subgraph
    graph = StateGraph(ResearchState)

    # Add research node
    graph.add_node("research", research_node)

    # Set entry and exit points
    graph.set_entry_point("research")
    graph.set_finish_point("research")

    # Compile and return
    logger.debug("‚úÖ Research subgraph v6.1 compiled")
    return graph.compile()
