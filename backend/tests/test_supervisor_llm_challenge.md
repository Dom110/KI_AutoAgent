# üîç Phase 3: Supervisor LLM Integration Challenge

**Datum:** 2025-11-10  
**Problem:** Strukturierte Outputs (Pydantic models) mit Factory Provider

---

## Das Problem

Current code (LangChain-spezifisch):
```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel

class SupervisorDecision(BaseModel):
    action: str
    reasoning: str
    confidence: float
    next_agent: str | None

# ‚ö†Ô∏è PROBLEM: .with_structured_output() ist LangChain-spezifisch!
decision = await self.llm.with_structured_output(
    SupervisorDecision
).ainvoke([
    SystemMessage(content=system_prompt),
    HumanMessage(content=user_prompt)
])
```

**Was macht das?**
1. LLM wird mit JSON Schema prompt angereichert
2. LLM wird gezwungen, strukturiertes JSON zu erzeugen
3. JSON wird parsed und als Pydantic-Modell validiert
4. Typsichere Zugriffe: `decision.action`, `decision.confidence`, etc.

**Mit Factory Provider (Problem):**
```python
# Factory gibt nur LLMResponse mit .content (string) zur√ºck
response = await self.llm_provider.generate_text_with_retries(
    prompt=prompt,
    system_prompt="..."
)
# response.content ist nur ein STRING!
# Wie bekomme ich ein strukturiertes SupervisorDecision Objekt?
```

---

## L√∂sungsans√§tze

### Option 1: JSON-Schema Prompt Engineering
```python
import json
from pydantic import BaseModel

def create_json_schema_prompt(model: type[BaseModel]) -> str:
    """Generate JSON schema prompt instruction."""
    schema = model.model_json_schema()
    return f"""
You MUST respond with ONLY valid JSON matching this schema:
{json.dumps(schema, indent=2)}

RESPOND WITH ONLY JSON, NO EXPLANATIONS.
"""

# Usage
schema_prompt = create_json_schema_prompt(SupervisorDecision)
full_prompt = schema_prompt + "\n\n" + original_prompt

response = await self.llm_provider.generate_text_with_retries(
    prompt=full_prompt,
    system_prompt="You are a JSON generator. Always respond with valid JSON."
)

# Parse the JSON response
try:
    json_data = json.loads(response.content)
    decision = SupervisorDecision(**json_data)
except (json.JSONDecodeError, ValueError) as e:
    logger.error(f"Failed to parse structured output: {e}")
    raise
```

**Pros:**
- Works with any provider
- Full control over parsing
- Can validate & retry

**Cons:**
- Verbose
- Error prone (JSON parsing)
- Multiple retries if invalid JSON

### Option 2: Provider-Level Structured Output
```python
# In LLMProvider base class
async def generate_structured_output(
    self,
    prompt: str,
    output_model: type[T],
    system_prompt: str = None,
    max_retries: int = 3
) -> T:
    """Generate text and parse as Pydantic model."""
    schema = output_model.model_json_schema()
    
    # Enrich prompt with schema
    json_prompt = self._add_json_schema(prompt, schema)
    
    # Call generate_text_with_retries
    response = await self.generate_text_with_retries(
        prompt=json_prompt,
        system_prompt=system_prompt or "Always respond with valid JSON",
        max_retries=max_retries
    )
    
    # Parse and validate
    try:
        import json
        json_data = json.loads(response.content)
        result = output_model(**json_data)
        logger.info(f"‚úÖ Structured output parsed: {result}")
        return result
    except Exception as e:
        logger.error(f"‚ùå Failed to parse structured output: {e}")
        raise

# Usage
decision = await self.llm_provider.generate_structured_output(
    prompt=prompt,
    output_model=SupervisorDecision,
    system_prompt=system_prompt
)
```

**Pros:**
- Clean API
- Reusable across all agents
- Works with all providers

**Cons:**
- Modifies base LLMProvider
- Still requires JSON parsing

### Option 3: Thin Wrapper Around Factory
```python
class SupervisorLLMAdapter:
    """Adapter for Supervisor LLM calls with structured output."""
    
    def __init__(self, agent_name: str = "supervisor"):
        self.agent_name = agent_name
        self.provider = AgentLLMFactory.get_provider_for_agent(agent_name)
    
    async def get_decision(self, context: SupervisorContext) -> SupervisorDecision:
        """Get structured decision from LLM."""
        logger.info(f"ü§ñ Requesting structured decision from {self.agent_name}")
        
        # Build prompt with schema
        prompt = self._build_prompt(context)
        schema_instruction = self._get_schema_instruction()
        
        full_prompt = schema_instruction + "\n" + prompt
        
        # Call LLM
        logger.info(f"üì§ Calling {self.provider.model}...")
        response = await self.provider.generate_text_with_retries(
            prompt=full_prompt,
            system_prompt=self._get_system_prompt(),
            max_retries=3
        )
        
        logger.info(f"‚úÖ Got response: {response.total_tokens} tokens")
        
        # Parse
        import json
        try:
            json_data = json.loads(response.content)
            decision = SupervisorDecision(**json_data)
            logger.info(f"‚úÖ Parsed decision: {decision.action}")
            return decision
        except Exception as e:
            logger.error(f"‚ùå Failed to parse: {e}")
            logger.debug(f"   Response was: {response.content[:200]}")
            raise

# Usage in supervisor_mcp.py
adapter = SupervisorLLMAdapter("supervisor")
decision = await adapter.get_decision(context)
```

**Pros:**
- Minimal changes to base LLMProvider
- Clean separation of concerns
- Easy to test

**Cons:**
- Additional abstraction layer
- Still needs JSON parsing

---

## Recommendation: OPTION 2

**Why:**
- ‚úÖ Most elegant - extends base LLMProvider
- ‚úÖ All agents can use `generate_structured_output()`
- ‚úÖ Type-safe with Pydantic
- ‚úÖ Can be tested independently
- ‚úÖ Retries work automatically (built-in)
- ‚úÖ Logging is consistent across all providers

**Implementation Path:**
1. Add `generate_structured_output()` to LLMProvider base class
2. Implement in OpenAIProvider + AnthropicProvider
3. Update SupervisorMCP to use it
4. Test with all providers
5. Document pattern in CLAUDE.md

---

## Next Steps

1. **Create test script** - test JSON schema generation
2. **Extend LLMProvider** - add `generate_structured_output()`
3. **Update SupervisorMCP** - use new method
4. **Test thoroughly** - unit + integration + E2E
5. **Document pattern** - for all agents to follow

