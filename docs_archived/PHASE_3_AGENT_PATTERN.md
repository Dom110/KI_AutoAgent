# ğŸ”„ Phase 3: Agent Integration Pattern (Reusable)

**Das Muster, das ALLE 6 Agents folgen sollen**

---

## Das Pattern (5 Schritte)

### Schritt 1: Imports aktualisieren

```python
# ENTFERNEN
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import SystemMessage, HumanMessage

# HINZUFÃœGEN
from backend.core.llm_factory import AgentLLMFactory
from backend.core.llm_config import AgentLLMConfigManager
from pathlib import Path
import logging

logger = logging.getLogger("agent.your_agent_name")
```

---

### Schritt 2: __init__() anpassen

```python
class YourAgent:
    # VORHER
    def __init__(self, model: str = "gpt-4o"):
        self.llm = ChatOpenAI(model=model, temperature=0.4)
    
    # NACHHER
    def __init__(self, agent_name: str = "codesmith"):
        """
        Initialize agent with Factory-based LLM.
        
        Args:
            agent_name: Name in agent_llm_config.json
        """
        logger.info(f"ğŸ¤– Initializing {agent_name}...")
        
        # Initialize config (once per app startup)
        config_path = Path("backend/config/agent_llm_config.json")
        AgentLLMConfigManager.initialize(config_path)
        
        # Get LLM provider from factory
        self.llm_provider = AgentLLMFactory.get_provider_for_agent(agent_name)
        
        # Log configuration
        logger.info(f"   âœ… Provider: {self.llm_provider.get_provider_name()}")
        logger.info(f"   âœ… Model: {self.llm_provider.model}")
        logger.info(f"   âœ… Temperature: {self.llm_provider.temperature}")
        logger.info(f"   âœ… Max tokens: {self.llm_provider.max_tokens}")
        logger.info(f"   âœ… Timeout: {self.llm_provider.timeout_seconds}s")
        
        self.agent_name = agent_name
        logger.info(f"âœ… {agent_name} initialized")
```

---

### Schritt 3: LLM-Aufrufe ersetzen

#### 3a: Einfache Text-Generierung

```python
# VORHER
async def generate_code(self, prompt: str) -> str:
    response = self.llm.invoke(prompt)
    return response.content

# NACHHER
async def generate_code(self, prompt: str) -> str:
    logger.info(f"ğŸ“¤ Generating code...")
    logger.debug(f"   Prompt: {prompt[:100]}...")
    
    try:
        response = await self.llm_provider.generate_text_with_retries(
            prompt=prompt,
            system_prompt="You are an expert code generator",
            max_retries=3
        )
        
        logger.info(f"âœ… Code generated")
        logger.info(f"   Tokens: {response.total_tokens}")
        logger.info(f"   Time: {response.response_time_ms}ms")
        logger.debug(f"   Content: {response.content[:200]}...")
        
        return response.content
        
    except Exception as e:
        logger.error(f"âŒ Code generation failed: {e}")
        raise
```

#### 3b: Strukturierte Outputs (Entscheidungen, JSON)

```python
from pydantic import BaseModel

class CodeReview(BaseModel):
    quality: int  # 1-10
    issues: list[str]
    recommendations: str
    approve: bool

# VORHER
async def review_code(self, code: str) -> dict:
    response = self.llm.with_structured_output(CodeReview).invoke(code)
    return response.dict()

# NACHHER
async def review_code(self, code: str) -> CodeReview:
    logger.info(f"ğŸ” Reviewing code...")
    
    try:
        review = await self.llm_provider.generate_structured_output(
            prompt=f"Review this code:\n{code}",
            output_model=CodeReview,
            system_prompt="You are a code reviewer. Respond with JSON.",
            max_retries=3
        )
        
        logger.info(f"âœ… Review complete")
        logger.info(f"   Quality: {review.quality}/10")
        logger.info(f"   Issues: {len(review.issues)}")
        logger.info(f"   Approve: {review.approve}")
        
        return review
        
    except ValueError as e:
        logger.error(f"âŒ Review parsing failed: {e}")
        raise
    except Exception as e:
        logger.error(f"âŒ Review generation failed: {e}")
        raise
```

---

### Schritt 4: Error-Handling standardisieren

```python
async def some_agent_method(self):
    """Template fÃ¼r alle LLM-Aufrufe."""
    
    logger.info(f"ğŸ“¤ Starting {self.agent_name} operation...")
    
    try:
        # Call LLM via provider
        result = await self.llm_provider.generate_text_with_retries(...)
        logger.info(f"âœ… Operation successful")
        return result
        
    except asyncio.TimeoutError:
        logger.error(f"âŒ LLM call timed out")
        logger.error(f"   Timeout: {self.llm_provider.timeout_seconds}s")
        raise
        
    except ValueError as e:
        logger.error(f"âŒ Invalid response format: {e}")
        raise
        
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {type(e).__name__}")
        logger.error(f"   Message: {str(e)}")
        logger.debug(f"   Full traceback:", exc_info=True)
        raise
```

---

### Schritt 5: Konfiguration in agent_llm_config.json

```json
{
  "agents": {
    "supervisor": {
      "provider": "openai",
      "model": "gpt-4o-2024-11-20",
      "temperature": 0.3,
      "max_tokens": 1500,
      "timeout_seconds": 30
    },
    "codesmith": {
      "provider": "anthropic",
      "model": "claude-3-5-sonnet-20241022",
      "temperature": 0.2,
      "max_tokens": 4000,
      "timeout_seconds": 60
    },
    "architect": {
      "provider": "anthropic",
      "model": "claude-opus-4-1",
      "temperature": 0.3,
      "max_tokens": 8000,
      "timeout_seconds": 60
    },
    "research": {
      "provider": "anthropic",
      "model": "claude-haiku-4",
      "temperature": 0.7,
      "max_tokens": 2000,
      "timeout_seconds": 45
    },
    "reviewfix": {
      "provider": "openai",
      "model": "gpt-4o-mini",
      "temperature": 0.2,
      "max_tokens": 2000,
      "timeout_seconds": 30
    },
    "responder": {
      "provider": "openai",
      "model": "gpt-4o-2024-11-20",
      "temperature": 0.5,
      "max_tokens": 1000,
      "timeout_seconds": 30
    }
  }
}
```

---

## Das Pattern Zusammengefasst

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 3 PATTERN                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. IMPORTS                                                â”‚
â”‚     ChatOpenAI/ChatAnthropic âŒ                            â”‚
â”‚     â†’ AgentLLMFactory âœ…                                   â”‚
â”‚                                                             â”‚
â”‚  2. __init__()                                             â”‚
â”‚     self.llm = ChatOpenAI(...) âŒ                          â”‚
â”‚     â†’ self.llm_provider = Factory.get_provider(...) âœ…     â”‚
â”‚     â†’ Logging: Provider + Model + Temp âœ…                 â”‚
â”‚                                                             â”‚
â”‚  3. LLM CALLS (Simple)                                    â”‚
â”‚     self.llm.invoke(...) âŒ                               â”‚
â”‚     â†’ await self.llm_provider.generate_text_with_retries(..) â”‚
â”‚                                                             â”‚
â”‚  3. LLM CALLS (Structured)                                â”‚
â”‚     self.llm.with_structured_output(...).invoke(...) âŒ    â”‚
â”‚     â†’ await self.llm_provider.generate_structured_output(...) â”‚
â”‚                                                             â”‚
â”‚  4. ERROR HANDLING                                         â”‚
â”‚     Standardisiert (asyncio, ValueError, Exception) âœ…     â”‚
â”‚                                                             â”‚
â”‚  5. CONFIG                                                 â”‚
â”‚     Alle Einstellungen in agent_llm_config.json âœ…         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Logging Pattern

### âœ… Was JEDER Agent loggen soll

```python
# 1. INIT
logger.info(f"ğŸ¤– Initializing {agent_name}...")
logger.info(f"   âœ… Provider: {provider_name}")
logger.info(f"   âœ… Model: {model}")

# 2. REQUEST
logger.info(f"ğŸ“¤ Calling LLM for {operation}...")
logger.debug(f"   Prompt: {prompt[:100]}...")

# 3. RESPONSE
logger.info(f"âœ… {operation} complete")
logger.info(f"   Tokens: {response.total_tokens}")
logger.info(f"   Time: {response.response_time_ms}ms")

# 4. ERROR
logger.error(f"âŒ {operation} failed: {error_type}")
logger.error(f"   Message: {error_msg}")
logger.debug(f"   Details:", exc_info=True)
```

### âŒ Was NICHT geloggt werden soll

```python
logger.info(f"API Key: {api_key}")  # âŒ SECRETS!
print(f"DEBUG: {var}")               # âŒ Use logger.debug()
logger.info("...")                   # âŒ Silent failures!
```

---

## Konkrete Beispiele fÃ¼r die 6 Agents

### 1ï¸âƒ£ Supervisor (bereits dokumentiert)
```
Datei: backend/core/supervisor_mcp.py
Pattern: Text + Structured Output (SupervisorDecision)
Config: openai:gpt-4o, temp=0.3
```

### 2ï¸âƒ£ Codesmith
```
Datei: backend/agents/specialized/codesmith_agent.py
Pattern: Structured Output (CodeQualityCheck, GeneratedCode)
Config: anthropic:claude-sonnet, temp=0.2
```

### 3ï¸âƒ£ Architect
```
Datei: backend/agents/specialized/architect_agent.py
Pattern: Structured Output (ArchitectureDecision)
Config: anthropic:claude-opus, temp=0.3
```

### 4ï¸âƒ£ Research
```
Datei: backend/agents/specialized/research_agent.py
Pattern: Simple Text (Web-Recherche)
Config: anthropic:claude-haiku, temp=0.7
```

### 5ï¸âƒ£ ReviewFix
```
Datei: backend/agents/specialized/reviewer_gpt_agent.py
Pattern: Structured Output (ReviewResult)
Config: openai:gpt-4o-mini, temp=0.2
```

### 6ï¸âƒ£ Responder
```
Datei: [needs to be found]
Pattern: Simple Text (User-Response)
Config: openai:gpt-4o, temp=0.5
```

---

## Checklist pro Agent

```python
âœ… Step 1: Imports aktualisiert?
   - ChatOpenAI/ChatAnthropic ENTFERNT?
   - AgentLLMFactory + AgentLLMConfigManager HINZUGEFÃœGT?

âœ… Step 2: __init__() angepasst?
   - Config laden via AgentLLMConfigManager?
   - Provider via Factory?
   - Logging fÃ¼r Provider + Model + Temp?

âœ… Step 3: LLM-Aufrufe ersetzt?
   - generate_text_with_retries() fÃ¼r einfache Text?
   - generate_structured_output() fÃ¼r strukturierte?
   - Alle async/await?

âœ… Step 4: Error-Handling standardisiert?
   - asyncio.TimeoutError?
   - ValueError (parsing)?
   - Generic Exception mit full traceback?

âœ… Step 5: Config in JSON?
   - Agent-Name richtig geschrieben?
   - Provider + Model korrekt?
   - Temperature/Tokens sensible Defaults?

âœ… Step 6: Tests?
   - Unit test: __init__() nutzt Factory?
   - Integration test: LLM-Call funktioniert?
   - E2E test: Agent funktioniert im Workflow?
   - Logging test: ğŸ¤–ğŸ“¤âœ…âŒ visible?
```

---

## Timing pro Agent

```
Pro Agent (durchschnittlich):
- Imports + __init__(): 15 min
- LLM-Aufrufe ersetzen: 20 min
- Error-Handling: 10 min
- Config JSON: 5 min
- Tests schreiben: 30 min
- Verifizieren + Logging: 20 min
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~100 min (1.5-2 Stunden) pro Agent

FÃ¼r alle 6 Agents:
- Supervisor: 2h (komplexer mit structured output)
- Codesmith: 1.5h
- Architect: 1.5h
- Research: 1h (einfacher, nur Text)
- ReviewFix: 1.5h
- Responder: 1h (einfacher)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~9 Stunden fÃ¼r alle Agents
```

---

## Success Criteria (pro Agent)

```
âœ… Code compiles ohne Errors
âœ… Imports sind korrekt (kein CircularImport)
âœ… __init__() zeigt Logging mit ğŸ¤–, âœ…
âœ… LLM-Calls zeigen ğŸ“¤ und âœ… mit Tokens + Time
âœ… Errors zeigen âŒ mit aussagekrÃ¤ftiger Meldung
âœ… Config in JSON, nicht hardcoded
âœ… Alle Tests âœ… passing
âœ… Keine API-Calls in Tests (mock only)
âœ… Keine Secrets in Logs
âœ… E2E-Test zeigt korrekte Reihenfolge
```

---

## Reihenfolge der Implementation

```
DAY 1:
  ğŸ”œ supervisor_mcp.py (2h) - CORE
  ğŸ”œ Tests schreiben (1h)
  ğŸ”œ Verify + Logging (0.5h)

DAY 2:
  ğŸ”œ codesmith_agent.py (1.5h)
  ğŸ”œ architect_agent.py (1.5h)
  ğŸ”œ Tests (1h)

DAY 3:
  ğŸ”œ research_agent.py (1h)
  ğŸ”œ reviewfix_agent.py (1.5h)
  ğŸ”œ responder_agent.py (1h)
  ğŸ”œ Tests (1h)

DAY 4:
  ğŸ”œ Full E2E Testing
  ğŸ”œ Performance Benchmarks
  ğŸ”œ Documentation Update
  ğŸ”œ Final Verification
```

---

## Questions? 

Falls bei irgendeinem Agent unklar, was zu tun ist:
1. Lese diese Datei + den Agent-File
2. Lese PHASE_3_SUPERVISOR_UPDATE_GUIDE.md (concrete example)
3. Vergleiche mit supervisor_mcp.py (updated version)
4. Folge der Checklist

**Das Pattern ist konsistent fÃ¼r ALLE Agents!**

